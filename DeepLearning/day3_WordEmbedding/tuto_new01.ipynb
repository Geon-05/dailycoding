{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 된 결과 : [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [sent.split() for sent in sentences]\n",
    "print('단어 토큰화 된 결과 :', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 15\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for sent in tokenized_sentences:\n",
    "    for word in sent:\n",
    "      word_list.append(word)\n",
    "\n",
    "word_counts = Counter(word_list)\n",
    "print('총 단어수 :', len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'great', 'best', 'amazing', 'stop', 'lies', 'pitiful', 'nerd', 'excellent', 'work', 'supreme', 'quality', 'bad', 'highly', 'respectable']\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도순으로 정렬\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 토큰, UNK 토큰을 고려한 단어 집합의 크기 : 17\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "\n",
    "for index, word in enumerate(vocab) :\n",
    "  word_to_index[word] = index + 2\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print('패딩 토큰, UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]\n"
     ]
    }
   ],
   "source": [
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "  encoded_X_data = []\n",
    "  for sent in tokenized_X_data:\n",
    "    index_sequences = []\n",
    "    for word in sent:\n",
    "      try:\n",
    "          index_sequences.append(word_to_index[word])\n",
    "      except KeyError:\n",
    "          index_sequences.append(word_to_index['<UNK>'])\n",
    "    encoded_X_data.append(index_sequences)\n",
    "  return encoded_X_data\n",
    "\n",
    "X_encoded = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print('최대 길이 :',max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과 :\n",
      "[[ 2  3  4  5]\n",
      " [ 6  7  0  0]\n",
      " [ 8  9  0  0]\n",
      " [10 11  0  0]\n",
      " [12 13  0  0]\n",
      " [14  0  0  0]\n",
      " [15 16  0  0]]\n",
      "\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sentences, max_len):\n",
    "  features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "  for index, sentence in enumerate(sentences):\n",
    "    if len(sentence) != 0:\n",
    "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "  return features\n",
    "\n",
    "X_train = pad_sequences(X_encoded, max_len=max_len)\n",
    "y_train = np.array(y_train)\n",
    "print('패딩 결과 :')\n",
    "print(X_train)\n",
    "print()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedded.shape == (배치 크기, 문장의 길이, 임베딩 벡터의 차원)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # flattend.shape == (배치 크기, 문장의 길이 × 임베딩 벡터의 차원)\n",
    "        flattened = self.flatten(embedded)\n",
    "\n",
    "        # output.shape == (배치 크기, 1)\n",
    "        output = self.fc(flattened)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_dim = 100\n",
    "simple_model = SimpleModel(vocab_size, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(simple_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.730022668838501\n",
      "Epoch 2, Loss: 0.5908015370368958\n",
      "Epoch 3, Loss: 0.46054807305336\n",
      "Epoch 4, Loss: 0.36260297894477844\n",
      "Epoch 5, Loss: 0.29497313499450684\n",
      "Epoch 6, Loss: 0.249746173620224\n",
      "Epoch 7, Loss: 0.21922817826271057\n",
      "Epoch 8, Loss: 0.19752807915210724\n",
      "Epoch 9, Loss: 0.18058766424655914\n",
      "Epoch 10, Loss: 0.1658887267112732\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = simple_model(inputs).view(-1) \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 3000000 keys>\n",
      "KeyedVectors<vector_size=100, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model)\n",
    "word2vec_model.vector_size = 100\n",
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행렬의 크기 : (17, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "print('임베딩 행렬의 크기 :', embedding_matrix.shape)\n",
    "display(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <PAD>를 위한 0번과 <UNK>를 위한 1번은 실제 단어가 아니므로 맵핑에서 제외\n",
    "for word, i in word_to_index.items():\n",
    "    if i > 2:\n",
    "      temp = get_vector(word)\n",
    "      if temp is not None:\n",
    "          embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "300\n",
      "[ 7.17773438e-02  2.08007812e-01 -2.84423828e-02  1.78710938e-01\n",
      "  1.32812500e-01 -9.96093750e-02  9.61914062e-02 -1.16699219e-01\n",
      " -8.54492188e-03  1.48437500e-01 -3.34472656e-02 -1.85546875e-01\n",
      "  4.10156250e-02 -8.98437500e-02  2.17285156e-02  6.93359375e-02\n",
      "  1.80664062e-01  2.22656250e-01 -1.00585938e-01 -6.93359375e-02\n",
      "  1.04427338e-04  1.60156250e-01  4.07714844e-02  7.37304688e-02\n",
      "  1.53320312e-01  6.78710938e-02 -1.03027344e-01  4.17480469e-02\n",
      "  4.27246094e-02 -1.10351562e-01 -6.68945312e-02  4.19921875e-02\n",
      "  2.50000000e-01  2.12890625e-01  1.59179688e-01  1.44653320e-02\n",
      " -4.88281250e-02  1.39770508e-02  3.55529785e-03  2.09960938e-01\n",
      "  1.52343750e-01 -7.32421875e-02  2.16796875e-01 -5.76171875e-02\n",
      " -2.84423828e-02 -3.60107422e-03  1.52343750e-01 -2.63671875e-02\n",
      "  2.13623047e-02 -1.51367188e-01  1.04003906e-01  3.18359375e-01\n",
      " -1.85546875e-01  3.68652344e-02 -1.10839844e-01 -3.17382812e-02\n",
      " -1.01562500e-01 -1.21093750e-01  3.22265625e-01 -7.32421875e-02\n",
      " -1.52343750e-01  2.67578125e-01 -1.50390625e-01 -1.23046875e-01\n",
      "  1.07910156e-01  6.68945312e-02 -2.13623047e-02 -1.00585938e-01\n",
      " -2.05078125e-01  1.17675781e-01  6.15234375e-02  6.78710938e-02\n",
      "  1.06933594e-01 -7.71484375e-02 -1.52343750e-01 -4.24194336e-03\n",
      " -1.45507812e-01  2.53906250e-01  4.80957031e-02  9.71679688e-02\n",
      " -8.36181641e-03  1.12792969e-01  5.34667969e-02  1.79443359e-02\n",
      " -5.63964844e-02 -3.30078125e-01 -9.76562500e-02  1.42578125e-01\n",
      " -1.37695312e-01  2.20947266e-02  1.00097656e-01 -5.71289062e-02\n",
      " -1.56250000e-01 -6.37817383e-03 -9.37500000e-02 -4.68750000e-02\n",
      "  8.59375000e-02  3.06640625e-01 -1.11328125e-01 -1.94335938e-01\n",
      " -2.08007812e-01  8.10546875e-02 -4.19921875e-02 -8.30078125e-02\n",
      " -1.04003906e-01  2.92968750e-01  2.39257812e-02 -3.85742188e-02\n",
      "  3.56445312e-02 -1.04980469e-01 -6.54296875e-02  2.79296875e-01\n",
      " -1.16210938e-01 -1.45874023e-02  3.84765625e-01 -7.81250000e-02\n",
      " -2.92968750e-02 -1.35742188e-01 -5.39550781e-02 -5.49316406e-02\n",
      " -8.10546875e-02 -2.88085938e-02  8.34960938e-02  2.73437500e-01\n",
      " -6.20117188e-02 -4.78515625e-02 -1.09252930e-02 -1.13769531e-01\n",
      " -1.09863281e-01  2.02148438e-01 -1.28906250e-01 -6.68945312e-02\n",
      " -2.67578125e-01  9.61914062e-02  1.04003906e-01 -1.69921875e-01\n",
      "  5.56640625e-02  1.54296875e-01  8.05664062e-02  2.19726562e-01\n",
      " -2.27539062e-01  1.10351562e-01 -8.11767578e-03 -5.63964844e-02\n",
      " -9.03320312e-02 -7.76367188e-02 -3.61328125e-02  3.61328125e-02\n",
      "  1.58203125e-01 -1.56250000e-01  2.26562500e-01  2.85156250e-01\n",
      " -5.51757812e-02  3.53515625e-01 -1.20605469e-01  1.05957031e-01\n",
      "  3.11279297e-02 -1.91406250e-01 -2.31445312e-01 -1.11816406e-01\n",
      "  2.38037109e-03  7.51953125e-02 -1.28784180e-02  1.00585938e-01\n",
      "  4.45312500e-01 -2.77343750e-01  6.68945312e-02 -8.10546875e-02\n",
      "  6.39648438e-02  1.85546875e-02 -1.11328125e-01  9.76562500e-02\n",
      "  2.06054688e-01 -1.30859375e-01  2.39257812e-02  1.10839844e-01\n",
      "  8.05664062e-02 -1.52343750e-01  4.85229492e-03  1.84326172e-02\n",
      " -9.17968750e-02 -2.41210938e-01  8.39843750e-02 -1.00585938e-01\n",
      " -1.54296875e-01  2.75878906e-02 -1.64062500e-01 -1.01562500e-01\n",
      " -6.07299805e-03  1.33514404e-03 -2.53906250e-01  3.14453125e-01\n",
      "  1.31835938e-01 -1.31835938e-01  2.17285156e-02 -1.56250000e-01\n",
      " -1.46484375e-01 -5.12695312e-02 -1.20605469e-01 -2.15820312e-01\n",
      "  3.10058594e-02  1.30859375e-01  9.71679688e-02  5.67626953e-03\n",
      "  2.20947266e-02  1.26953125e-01 -1.24511719e-02  6.15234375e-02\n",
      " -2.23388672e-02  2.50000000e-01 -7.17773438e-02  1.58203125e-01\n",
      " -7.27539062e-02  1.97753906e-02  8.85009766e-03 -9.08203125e-02\n",
      "  3.63281250e-01 -9.03320312e-02  2.41699219e-02 -1.39770508e-02\n",
      " -5.10253906e-02  2.40478516e-02  5.88989258e-03 -1.02050781e-01\n",
      " -8.85009766e-03  3.05175781e-02 -7.81250000e-02 -1.27929688e-01\n",
      "  3.85742188e-02  2.86865234e-02 -2.28515625e-01 -1.25122070e-02\n",
      "  1.54296875e-01  9.13085938e-02  1.05468750e-01 -6.44531250e-02\n",
      " -1.28906250e-01 -1.02050781e-01 -2.16064453e-02 -3.29589844e-02\n",
      "  7.47070312e-02  3.78417969e-02  7.42187500e-02 -1.23901367e-02\n",
      " -4.68750000e-02  4.88281250e-03  1.03515625e-01 -8.69140625e-02\n",
      " -2.26562500e-01 -2.53906250e-01  3.58886719e-02  4.45312500e-01\n",
      "  5.56640625e-02  1.59179688e-01  2.71484375e-01 -1.08398438e-01\n",
      "  6.25000000e-02 -5.59082031e-02 -2.50000000e-01 -1.55273438e-01\n",
      " -6.83593750e-02 -1.39648438e-01 -1.59179688e-01 -1.79443359e-02\n",
      "  2.12402344e-02  7.37304688e-02  1.30859375e-01 -8.05664062e-02\n",
      "  2.99072266e-02  1.55639648e-02 -1.66015625e-01  1.50390625e-01\n",
      " -6.77490234e-03  1.01318359e-02  1.14746094e-01 -1.48437500e-01\n",
      " -4.58984375e-02 -1.39648438e-01 -1.73828125e-01 -4.27246094e-02\n",
      " -5.81054688e-02  5.22460938e-02 -1.11328125e-01  8.44726562e-02\n",
      " -2.55126953e-02  1.40625000e-01 -1.81640625e-01  1.72119141e-02\n",
      " -1.37695312e-01 -1.47705078e-02 -1.14746094e-02  6.44531250e-02\n",
      " -2.89062500e-01 -4.80957031e-02 -1.99218750e-01 -7.12890625e-02\n",
      "  6.44531250e-02 -1.67968750e-01 -2.08740234e-02 -1.42578125e-01]\n"
     ]
    }
   ],
   "source": [
    "# <PAD>나 <UNK>의 경우는 사전 훈련된 임베딩이 들어가지 않아서 0벡터임\n",
    "print(embedding_matrix[0])\n",
    "print(len(embedding_matrix[0]))\n",
    "print(embedding_matrix[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'nice': 2,\n",
       " 'great': 3,\n",
       " 'best': 4,\n",
       " 'amazing': 5,\n",
       " 'stop': 6,\n",
       " 'lies': 7,\n",
       " 'pitiful': 8,\n",
       " 'nerd': 9,\n",
       " 'excellent': 10,\n",
       " 'work': 11,\n",
       " 'supreme': 12,\n",
       " 'quality': 13,\n",
       " 'bad': 14,\n",
       " 'highly': 15,\n",
       " 'respectable': 16}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec_model에서 'great'의 임베딩 벡터\n",
    "# embedding_matrix[3]이 일치하는지 체크\n",
    "np.all(word2vec_model['great'] == embedding_matrix[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(PretrainedEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        flattened = self.flatten(embedded)\n",
    "        output = self.fc(flattened)\n",
    "        return self.sigmoid(output)\n",
    "    \n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedded.shape == (배치 크기, 문장의 길이, 임베딩 벡터의 차원)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # flattend.shape == (배치 크기, 문장의 길이 × 임베딩 벡터의 차원)\n",
    "        flattened = self.flatten(embedded)\n",
    "\n",
    "        # output.shape == (배치 크기, 1)\n",
    "        output = self.fc(flattened)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraiend_embedding_model = PretrainedEmbeddingModel(vocab_size, 300).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(pretraiend_embedding_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4, 5],\n",
       "        [6, 7, 0, 0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.669833242893219\n",
      "Epoch 2, Loss: 0.608708381652832\n",
      "Epoch 3, Loss: 0.5479153990745544\n",
      "Epoch 4, Loss: 0.4907554090023041\n",
      "Epoch 5, Loss: 0.4381197988986969\n",
      "Epoch 6, Loss: 0.3902027904987335\n",
      "Epoch 7, Loss: 0.3469182550907135\n",
      "Epoch 8, Loss: 0.3080556392669678\n",
      "Epoch 9, Loss: 0.273343950510025\n",
      "Epoch 10, Loss: 0.24248185753822327\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # inputs.shape == (배치 크기, 문장 길이)\n",
    "        # targets.shape == (배치 크기)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # outputs.shape == (배치 크기)\n",
    "        outputs = pretraiend_embedding_model(inputs).view(-1) \n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studypy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
